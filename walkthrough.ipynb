{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -Ls https://astral.sh/uv/install.sh | sh\n",
    "!git clone https://github.com/alrazol/GridControl.git\n",
    "!cd GridControl && uv pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "DB_URL=\"sqlite:///grid.db\"\n",
    "SHOULD_CREATE_TABLES=True\n",
    "NETWORK_API_BASEURL=\"http://localhost:8000\"\n",
    "ARTIFACTS_LOCATION=\"data/experiments\"\n",
    "MLFLOW_TRACKING_URI=\"sqlite:///mlflow_backend_store.db\"\n",
    "MLFLOW_PORT=8080\n",
    "LOG_LEVEL=INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from src.core.domain.use_cases.import_network_from_json import ETLPipeline\n",
    "from src.core.infrastructure.settings import Settings\n",
    "from src.core.infrastructure.adapters.sqlite_network_repository import (\n",
    "    SQLiteNetworkRepository,\n",
    ")\n",
    "from src.core.infrastructure.adapters.network_builder import DefaultNetworkBuilder\n",
    "from src.core.domain.use_cases.compute_simulated_network import SimulationPipeline\n",
    "from src.core.infrastructure.services import PyPowsyblCompatService\n",
    "from src.core.infrastructure.adapters.pypowsybl_loadflow_solver import PyPowSyblLoadFlowSolver\n",
    "from src.core.constants import LoadFlowType\n",
    "from src.core.utils import parse_datetime_to_str\n",
    "from src.core.constants import DEFAULT_TIMEZONE\n",
    "from src.rl.artifacts.experiment_record import ExperimentRecord\n",
    "import src.rl.action as action_module\n",
    "from src.rl.action.enums import DiscreteActionTypes\n",
    "from src.core.constants import ElementStatus\n",
    "from src.rl.train import train\n",
    "from src.rl.agent import DoNothingAgent\n",
    "from src.rl.config_loaders.agent.config_loader import AgentConfig\n",
    "from src.core.constants import LoadFlowType\n",
    "from src.rl.repositories import Repositories\n",
    "from src.rl.config_loaders.environment.config_loader import EnvironmentConfig\n",
    "from src.rl.environment import make_env\n",
    "\n",
    "PATH_TO_LAYOUT = \"configs/toy_grid_layout.json\"\n",
    "SIMULATION_CONFIG_PATH = Path(\"configs/toy_grid_simulation.yaml\")\n",
    "GRID_ID = \"toy_grid_layout\"\n",
    "SHOULD_CREATE_TABLE = True\n",
    "settings = Settings()\n",
    "network_builder = DefaultNetworkBuilder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Grid Layout and Scenario Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Grid Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a start, we want to focus on easily interpretable grids with a friendly JSON format. We therefore implemented a way to define and read from JSON the layout of a grid. In this section we will visualise the raw JSON as we defined it for a very simple grid, made of one load, one generator, and two lines. Agnostic from this file format, we want to read from it and ingest the data into a database. The goal here is to support all kinds of file formats, and have one single repository that is agnostic of the source format where we can then query our grids from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the raw JSON we used to define the grid layout\n",
    "\n",
    "with open(PATH_TO_LAYOUT, \"r\") as f:\n",
    "    layout_json = json.load(f)\n",
    "\n",
    "print(json.dumps(layout_json, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the data into the DB repository\n",
    "\n",
    "network_repository = SQLiteNetworkRepository(\n",
    "    should_create_tables=SHOULD_CREATE_TABLE,\n",
    "    db_url=settings.DB_URL,\n",
    ")\n",
    "\n",
    "etl_pipeline = ETLPipeline(\n",
    "    network_repository=network_repository,\n",
    "    network_builder=network_builder,\n",
    ")\n",
    "etl_pipeline.run(file_path=PATH_TO_LAYOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the elements of the ingested network\n",
    "\n",
    "net = network_repository.get(network_id=GRID_ID)\n",
    "net.elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Simulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the \"static\" definition of out network, we want to go to a \"dynamic\" definition, where we simulate some \"dynamic\" parameters of the network over an arbitrary number of timestamps. In order to achieve that, we define some simulation configuration with some simulation methods that can be used to adjust the trajectory of our elements through time. We first print the simulation yaml configutation, and then run the simulation. The resulting \"dynamic\" elements (with some timestamp dependent variables) are ingested in the DB repository as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the raw simulation yaml config\n",
    "\n",
    "with open(SIMULATION_CONFIG_PATH, \"r\") as f:\n",
    "    simulation_yaml = f.read()\n",
    "\n",
    "print(simulation_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a time series of the network\n",
    "\n",
    "START = datetime(2025, 1, 1, tzinfo=DEFAULT_TIMEZONE)\n",
    "END = datetime(2025, 2, 1, tzinfo=DEFAULT_TIMEZONE)\n",
    "\n",
    "simulation_pipeline = SimulationPipeline(\n",
    "    config_path=SIMULATION_CONFIG_PATH,\n",
    "    network_repository=network_repository,\n",
    "    network_builder=network_builder,\n",
    ")\n",
    "\n",
    "simulation_pipeline.apply_pipeline(start=parse_datetime_to_str(START), end=parse_datetime_to_str(END), time_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_simulated = network_repository.get(network_id=f\"{GRID_ID}_simulated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the dynamic parameters for the generator and the load in the network over time\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        net_simulated.to_dataframe(element_id=\"gen1\")[[\"dynamic.Ptarget\"]],\n",
    "        net_simulated.to_dataframe(element_id=\"gen1\")[[\"static.Pmax\"]],\n",
    "        net_simulated.to_dataframe(element_id=\"load1\")[[\"dynamic.Pd\"]],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "px.line(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on those dynamic parameters, solve a DC load flow for the network at each time step. The solve can potentially fail if the defintions are inconsistent\n",
    "\n",
    "solver = PyPowSyblLoadFlowSolver(\n",
    "    to_pypowsybl_converter_service=PyPowsyblCompatService(),\n",
    "    network_builder=network_builder,\n",
    ")\n",
    "\n",
    "net_solved = solver.solve(network=net_simulated, loadflow_type=LoadFlowType.DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the flow in the lines, as well as the active power injected by the generator and consumed by the load after the load flow solve\n",
    "# We notive that as we have two lines active, the power injected by the generator is split across both lines\n",
    "\n",
    "ELEMENT_ID_1 = \"gen1\"\n",
    "ELEMENT_ID_2 = \"load1\"\n",
    "line_1_id = \"line1\"\n",
    "\n",
    "df_solved= pd.concat(\n",
    "    [\n",
    "        net_solved.to_dataframe(element_id=ELEMENT_ID_1)[[\"solved.p\"]].rename(columns={\"solved.p\": f\"solved.p_{ELEMENT_ID_1}\"}),\n",
    "        net_solved.to_dataframe(element_id=\"load1\")[[\"solved.p\"]].rename(columns={\"solved.p\": f\"solved.p_{ELEMENT_ID_2}\"}),\n",
    "        net_solved.to_dataframe(element_id=line_1_id)[[\"solved.p1\"]]\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "px.line(df_solved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Training an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_CONFIG_PATH = \"GridControl/src/rl/configs/agent/do_nothing_agent.yaml\"\n",
    "ENVIRONMENT_CONFIG_PATH = \"GridControl/src/rl/configs/environment/with_outage.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise raw agent yaml config\n",
    "\n",
    "with open(AGENT_CONFIG_PATH, \"r\") as f:\n",
    "    agent_yaml = f.read()\n",
    "print(agent_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise raw environment yaml config\n",
    "\n",
    "with open(ENVIRONMENT_CONFIG_PATH, \"r\") as f:\n",
    "    environment_yaml = f.read()\n",
    "print(environment_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configs and setup paths and dependencies\n",
    "\n",
    "agent_config = AgentConfig.from_yaml(config_path=Path(AGENT_CONFIG_PATH))\n",
    "environment_config = EnvironmentConfig.from_yaml(\n",
    "    config_path=Path(ENVIRONMENT_CONFIG_PATH),\n",
    ")\n",
    "repositories = Repositories(s=settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the env\n",
    "\n",
    "\n",
    "env = make_env(\n",
    "    network_id=f\"{GRID_ID}_simulated\",\n",
    "    network_repository=repositories.get_network_repository(),\n",
    "    environment_config=environment_config,\n",
    "    loadflow_solver=repositories.get_solver(),\n",
    "    network_builder=repositories.get_network_builder(),\n",
    "    network_snapshot_observation_builder=repositories.get_network_snapshot_observation_builder(\n",
    "        class_name=environment_config.network_snapshot_builder\n",
    "    ),\n",
    "    action_space_builder=repositories.get_action_space_builder(),\n",
    "    one_hot_map_builder=repositories.get_one_hot_map_builder(\n",
    "        class_name=environment_config.one_hot_map_builder\n",
    "    ),\n",
    "    network_observation_handler=repositories.get_network_observation_handler(),\n",
    "    network_transition_handler=repositories.get_network_transition_handler(\n",
    "        class_name=environment_config.network_transition_handler\n",
    "    ),\n",
    "    loadflow_type=LoadFlowType.DC,\n",
    "    reward_handler=repositories.get_reward_handler(\n",
    "        aggregator_name=agent_config.rewards.get(\"rewards_aggregator\"),\n",
    "        rewards=agent_config.rewards.get(\"rewards\"),\n",
    "    ),\n",
    "    action_types=agent_config.action_types,\n",
    "    observation_memory_length=agent_config.hyperparameters.get(\n",
    "        \"observation_memory_length\"\n",
    "    ),\n",
    "    outage_handler_builder=repositories.get_outage_handler_builder(),\n",
    "    network_element_outage_handler_builder=repositories.get_network_element_outage_handler_builder(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) DoNothingAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"DoNothingAgent\" can't take any action and simply accumulates rewards based on what happends on the grid. By logging the rollout of the episodes they are exposed to, we can get a sense of the dynamics of the environment. Our environment is a stochastic one, where we apply some outages to the lines with some probability distribution that we defined.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    experiment_name=\"do_nothing_agent_toy_grid\",\n",
    "    env=env,\n",
    "    agent=DoNothingAgent(observation_memory_length=1),\n",
    "    action_space_builder=repositories.get_action_space_builder(),\n",
    "    num_episodes=10,\n",
    "    num_timesteps=100,\n",
    "    timestep_to_start_updating=20,\n",
    "    timestep_update_freq=10,\n",
    "    artifacts_location=settings.ARTIFACTS_LOCATION,\n",
    "    loss_tracker=repositories.get_loss_tracker(),\n",
    "    reward_tracker=repositories.get_reward_tracker(),\n",
    "    log_model=False,\n",
    "    log_rollout_freq=2,\n",
    "    registered_model_name=None,\n",
    "    seed=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise an episode from the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise an RL experiment layout\n",
    "\n",
    "ROLLOUT_PATH = \"src/rl/data/experiments/DQN-agent-exp/4716cdfe142645d99ade9b0207670e6f/artifacts/DQN-agent-exp_rollout_episode_250.json\"\n",
    "\n",
    "with open(Path(ROLLOUT_PATH), \"r\") as f:\n",
    "    rl_experiment_layout = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [ExperimentRecord(**i) for i in rl_experiment_layout[\"records\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(pd.DataFrame([j\n",
    " for record in records\n",
    " for j in record.next_observation[\"network_snapshot_observations\"][-1][\"observations\"]\n",
    " if j[\"id\"] == \"line2\"])[[\"timestamp\", \"p1\"]].set_index(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_status(status: ElementStatus) -> int:\n",
    "    if status == ElementStatus.ON:\n",
    "        return 0\n",
    "    elif status == ElementStatus.OFF:\n",
    "        return 1\n",
    "    elif status == ElementStatus.OUTAGE:\n",
    "        return 2\n",
    "    elif status == ElementStatus.MAINTENANCE:\n",
    "        return 3\n",
    "\n",
    "df_status = pd.DataFrame([j\n",
    " for record in records\n",
    " for j in record.next_observation[\"network_snapshot_observations\"][-1][\"observations\"]\n",
    " if j[\"id\"] == \"line1\"])[[\"timestamp\", \"status\"]].set_index(\"timestamp\")\n",
    "\n",
    "df_status[\"status_code\"] = df_status[\"status\"].apply(assign_status)\n",
    "\n",
    "px.line(df_status[[\"status_code\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(pd.DataFrame([i.reward for i in records]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_action(action: dict, action_type: DiscreteActionTypes) -> int:\n",
    "    try:\n",
    "        action_class = getattr(action_module, action_type)\n",
    "        if action_type == \"DoNothingAction\":\n",
    "            action = {}\n",
    "        if action_type == \"SwitchAction\":\n",
    "            action = {\"element_id\": action[\"element_id\"]}\n",
    "        if action_type == \"StartMaintenanceAction\":\n",
    "            action = {\"element_id\": action[\"element_id\"]}\n",
    "        action = action_class(**action)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Unknown action type: {action_type}: {e}\")\n",
    "    if isinstance(action, action_module.DoNothingAction):\n",
    "        return 0\n",
    "    elif isinstance(action, action_module.SwitchAction) and action.element_id == \"line1\":\n",
    "        return 1\n",
    "    elif isinstance(action, action_module.SwitchAction) and action.element_id == \"line2\":\n",
    "        return 2\n",
    "    elif isinstance(action, action_module.StartMaintenanceAction) and action.element_id == \"line1\":\n",
    "        return 3\n",
    "    elif isinstance(action, action_module.StartMaintenanceAction) and action.element_id == \"line2\":\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(pd.DataFrame([assign_action(i.action, action_type=i.action[\"action_type\"]) for i in records]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
